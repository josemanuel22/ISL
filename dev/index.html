<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · ISL</title><meta name="title" content="Home · ISL"/><meta property="og:title" content="Home · ISL"/><meta property="twitter:title" content="Home · ISL"/><meta name="description" content="Documentation for ISL."/><meta property="og:description" content="Documentation for ISL."/><meta property="twitter:description" content="Documentation for ISL."/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>ISL</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a></li><li><a class="tocitem" href="gan/">GAN</a></li><li><a class="tocitem" href="example/">Example</a></li><li><a class="tocitem" href="benchmark/">Benchmark</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/josemanuel22/ISL" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/josemanuel22/ISL/blob/main/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="ISL"><a class="docs-heading-anchor" href="#ISL">ISL</a><a id="ISL-1"></a><a class="docs-heading-anchor-permalink" href="#ISL" title="Permalink"></a></h1><p>Documentation for ISL.jl</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ISL.AutoISLParams" href="#ISL.AutoISLParams"><code>ISL.AutoISLParams</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AutoISLParams</code></pre><p>Hyperparameters for the method <code>invariant_statistical_loss</code></p><pre><code class="language-julia hljs">@with_kw struct AutoISLParams
    samples::Int64 = 1000
    epochs::Int64 = 100
    η::Float64 = 1e-3
    max_k::Int64 = 10
    transform = Normal(0.0f0, 1.0f0)
end;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/josemanuel22/ISL/blob/38691646a98446e64cd307add9553544e578f187/src/CustomLossFunction.jl#L247-L261">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ISL.HyperParamsTS" href="#ISL.HyperParamsTS"><code>ISL.HyperParamsTS</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HyperParamsTS</code></pre><p>Hyperparameters for the method <code>ts_adaptative_block_learning</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/josemanuel22/ISL/blob/38691646a98446e64cd307add9553544e578f187/src/CustomLossFunction.jl#L403-L407">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ISL.ISLParams" href="#ISL.ISLParams"><code>ISL.ISLParams</code></a> — <span class="docstring-category">Type</span></header><section><div><p>ISLParams</p><p>Hyperparameters for the method <code>adaptative_block_learning</code></p><pre><code class="language-julia hljs">@with_kw struct ISLParams
    samples::Int64 = 1000               # number of samples per histogram
    K::Int64 = 2                        # number of simulted observations
    epochs::Int64 = 100                 # number of epochs
    η::Float64 = 1e-3                   # learning rate
    transform = Normal(0.0f0, 1.0f0)    # transform to apply to the data
end;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/josemanuel22/ISL/blob/38691646a98446e64cd307add9553544e578f187/src/CustomLossFunction.jl#L172-L186">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ISL._sigmoid-Union{Tuple{T}, Tuple{Matrix{T}, T}} where T&lt;:AbstractFloat" href="#ISL._sigmoid-Union{Tuple{T}, Tuple{Matrix{T}, T}} where T&lt;:AbstractFloat"><code>ISL._sigmoid</code></a> — <span class="docstring-category">Method</span></header><section><div><p><code>_sigmoid(ŷ::Matrix{T}, y::T) where {T&lt;:AbstractFloat}</code></p><p>Calculate the sigmoid function centered at <code>y</code>.</p><p><strong>Arguments</strong></p><ul><li><code>ŷ::Matrix{T}</code>: The matrix of values to apply the sigmoid function to.</li><li><code>y::T</code>: The center value around which the sigmoid function is centered.</li></ul><p><strong>Returns</strong></p><p>A matrix of the same size as <code>ŷ</code> containing the sigmoid-transformed values.</p><p>This function calculates the sigmoid function for each element in the matrix <code>ŷ</code> centered at the value <code>y</code>. It applies a fast sigmoid transformation with a scaling factor of 10.0.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/josemanuel22/ISL/blob/38691646a98446e64cd307add9553544e578f187/src/CustomLossFunction.jl#L1-L15">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ISL.auto_invariant_statistical_loss-Tuple{Any, Any, Any}" href="#ISL.auto_invariant_statistical_loss-Tuple{Any, Any, Any}"><code>ISL.auto_invariant_statistical_loss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">`auto_invariant_statistical_loss(model, data, hparams)``</code></pre><p>Custom loss function for the model.</p><p>This method gradually adapts <code>K</code> (starting from <code>2</code>) up to <code>max_k</code> (inclusive). The value of <code>K</code> is chosen based on a simple two-sample test between the histogram associated with the obtained result and the uniform distribution.</p><p>To see the value of <code>K</code> used in the test, set the logger level to debug before executing.</p><p>#Arguments</p><ul><li><code>model::Flux.Chain</code>: is a Flux neuronal network model</li><li><code>data::Flux.DataLoader</code>: is a loader Flux object</li><li><code>hparams::AutoAdaptativeHyperParams</code>: is a AutoAdaptativeHyperParams object</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/josemanuel22/ISL/blob/38691646a98446e64cd307add9553544e578f187/src/CustomLossFunction.jl#L301-L316">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ISL.convergence_to_uniform-Union{Tuple{Vector{T}}, Tuple{T}} where T&lt;:Int64" href="#ISL.convergence_to_uniform-Union{Tuple{Vector{T}}, Tuple{T}} where T&lt;:Int64"><code>ISL.convergence_to_uniform</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">`convergence_to_uniform(aₖ)``</code></pre><p>Test the convergence of the distributino of the window of the rv&#39;s <code>Aₖ</code> to a uniform distribution. It is implemented using a Chi-Square test.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/josemanuel22/ISL/blob/38691646a98446e64cd307add9553544e578f187/src/CustomLossFunction.jl#L280-L285">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ISL.generate_aₖ-Union{Tuple{T}, Tuple{Matrix{T}, T}} where T&lt;:AbstractFloat" href="#ISL.generate_aₖ-Union{Tuple{T}, Tuple{Matrix{T}, T}} where T&lt;:AbstractFloat"><code>ISL.generate_aₖ</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">`generate_aₖ(ŷ, y)``</code></pre><p>Calculate the values of the real observation <code>y</code> in each of the components of the approximate histogram with <code>K</code> bins.</p><p><strong>Arguments</strong></p><ul><li><code>ŷ::Matrix{T}</code>: A matrix of simulated observations (each column represents a different simulation).</li><li><code>y::T</code>: The real data for which the one-step histogram is generated.</li></ul><p><strong>Returns</strong></p><ul><li><code>aₖ::Vector{Float}</code>: A vector with the values of the real observation <code>y</code> in each of the components of the approximate histogram with <code>K</code> bins.</li></ul><p><strong>Details</strong></p><p>The <code>generate_aₖ</code> function calculates the one-step histogram <code>aₖ</code> as the sum of the contribution of the observation to the subrogate histogram bins. It uses the function <code>γ</code> to calculate the contribution of each observation at each histogram bin. The final histogram is the sum of these contributions.</p><p>The formula for generating <code>aₖ</code> is as follows:</p><p class="math-container">\[aₖ = ∑_{k=0}^K γ(ŷ, y, k) = ∑_{k=0}^K ∑_{i=1}^N ψₖ(ŷ, yᵢ)\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/josemanuel22/ISL/blob/38691646a98446e64cd307add9553544e578f187/src/CustomLossFunction.jl#L121-L142">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ISL.get_window_of_Aₖ-Tuple{Any, Any, Any, Int64}" href="#ISL.get_window_of_Aₖ-Tuple{Any, Any, Any, Int64}"><code>ISL.get_window_of_Aₖ</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">`get_window_of_Aₖ(model, target , K, n_samples)``</code></pre><p>Generate a window of the rv&#39;s <code>Aₖ</code> for a given model and target function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/josemanuel22/ISL/blob/38691646a98446e64cd307add9553544e578f187/src/CustomLossFunction.jl#L270-L274">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ISL.invariant_statistical_loss-Tuple{Any, Any, Any}" href="#ISL.invariant_statistical_loss-Tuple{Any, Any, Any}"><code>ISL.invariant_statistical_loss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">`invariant_statistical_loss(model, data, hparams)``</code></pre><p>Custom loss function for the model. model is a Flux neuronal network model, data is a loader Flux object and hparams is a HyperParams object.</p><p><strong>Arguments</strong></p><ul><li>nn_model::Flux.Chain: is a Flux neuronal network model</li><li>data::Flux.DataLoader: is a loader Flux object</li><li>hparams::HyperParams: is a HyperParams object</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/josemanuel22/ISL/blob/38691646a98446e64cd307add9553544e578f187/src/CustomLossFunction.jl#L195-L205">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ISL.jensen_shannon_∇-Union{Tuple{Vector{T}}, Tuple{T}} where T&lt;:AbstractFloat" href="#ISL.jensen_shannon_∇-Union{Tuple{Vector{T}}, Tuple{T}} where T&lt;:AbstractFloat"><code>ISL.jensen_shannon_∇</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">`jensen_shannon_∇(aₖ)``</code></pre><p>Jensen shannon difference between <code>aₖ</code> vector and uniform distribution vector.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/josemanuel22/ISL/blob/38691646a98446e64cd307add9553544e578f187/src/CustomLossFunction.jl#L158-L162">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ISL.scalar_diff-Union{Tuple{Vector{T}}, Tuple{T}} where T&lt;:AbstractFloat" href="#ISL.scalar_diff-Union{Tuple{Vector{T}}, Tuple{T}} where T&lt;:AbstractFloat"><code>ISL.scalar_diff</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">`scalar_diff(q)`</code></pre><p>Scalar difference between the vector representing our subrogate histogram and the uniform distribution vector.</p><p class="math-container">\[loss = ||q-1/k+1||_{2} = ∑_{k=0}^K (qₖ - 1/K+1)^2\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/josemanuel22/ISL/blob/38691646a98446e64cd307add9553544e578f187/src/CustomLossFunction.jl#L147-L155">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ISL.ts_invariant_statistical_loss-NTuple{5, Any}" href="#ISL.ts_invariant_statistical_loss-NTuple{5, Any}"><code>ISL.ts_invariant_statistical_loss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ts_invariant_statistical_loss(rec, gen, Xₜ, Xₜ₊₁, hparams)</code></pre><p>Train a model for time series data with statistical invariance loss method.</p><p>#Arguments</p><ul><li><code>rec</code>: The recurrent neural network (RNN) responsible for encoding the time series data.</li><li><code>gen</code>: The generative model used for generating future time series data.</li><li><code>Xₜ</code>: An array of input time series data at time <code>t</code>.</li><li><code>Xₜ₊₁</code>: An array of target time series data at time <code>t+1</code>.</li><li><code>hparams::NamedTuple</code>: A structure containing hyperparameters for training. It should include the following fields:<ul><li><code>η::Float64</code>: Learning rate for optimization.</li><li><code>window_size::Int</code>: Size of the sliding window used during training.</li><li><code>K::Int</code>: Number of samples in the generative model.</li><li><code>noise_model</code>: Noise model used for generating random noise.</li></ul></li></ul><p>#Returns</p><ul><li><code>losses::Vector{Float64}</code>: A vector containing the training loss values for each iteration.</li></ul><p>#Description This function train a model for time series data with statistical invariance loss method. It utilizes a recurrent neural network (<code>rec</code>) to encode the time series data at time <code>t</code> and a generative model (<code>gen</code>) to generate future time series data at time <code>t+1</code>. The training process involves optimizing both the <code>rec</code> and <code>gen</code> models.</p><p>The function iterates through the provided time series data (<code>Xₜ</code> and <code>Xₜ₊₁</code>) in batches, with a sliding window of size <code>window_size</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/josemanuel22/ISL/blob/38691646a98446e64cd307add9553544e578f187/src/CustomLossFunction.jl#L460-L484">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ISL.γ-Union{Tuple{T}, Tuple{Matrix{T}, T, Int64}} where T&lt;:AbstractFloat" href="#ISL.γ-Union{Tuple{T}, Tuple{Matrix{T}, T, Int64}} where T&lt;:AbstractFloat"><code>ISL.γ</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">γ(yₖ::Matrix{T}, yₙ::T, m::Int64) where {T&lt;:AbstractFloat}</code></pre><p>Calculate the contribution of <code>ψₘ ∘ ϕ(yₖ, yₙ)</code> to the <code>m</code> bin of the histogram as a Vector{Float}.</p><p><strong>Arguments</strong></p><ul><li><code>yₖ::Matrix{T}</code>: A matrix of values for which to compute the contribution.</li><li><code>yₙ::T</code>: The center value around which the sigmoid and bump functions are centered.</li><li><code>m::Int64</code>: The bin index for which to calculate the contribution.</li></ul><p><strong>Returns</strong></p><p>A vector of floating-point values representing the contribution to the <code>m</code> bin of the histogram.</p><p>This function calculates the contribution of the composition of <code>ψₘ</code> and <code>ϕ(yₖ, yₙ)</code> to the <code>m</code>-th bin of the histogram. The result is a vector of floating-point values.</p><p>The contribution is computed according to the formula:</p><p class="math-container">\[γ(yₖ, yₙ, m) = ψₘ ∘ ϕ(yₖ, yₙ)\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/josemanuel22/ISL/blob/38691646a98446e64cd307add9553544e578f187/src/CustomLossFunction.jl#L67-L87">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ISL.γ_fast-Union{Tuple{T}, Tuple{Matrix{T}, T, Int64}} where T&lt;:AbstractFloat" href="#ISL.γ_fast-Union{Tuple{T}, Tuple{Matrix{T}, T, Int64}} where T&lt;:AbstractFloat"><code>ISL.γ_fast</code></a> — <span class="docstring-category">Method</span></header><section><div><p><code>γ_fast(yₖ::Matrix{T}, yₙ::T, m::Int64) where {T&lt;:AbstractFloat}</code>`</p><p>Apply the <code>γ</code> function to the given parameters using StaticArrays for improved performance.</p><p><strong>Arguments</strong></p><ul><li><code>yₖ::Matrix{T}</code>: A matrix of values for which to compute the contribution.</li><li><code>yₙ::T</code>: The center value around which the sigmoid and bump functions are centered.</li><li><code>m::Int64</code>: The bin index for which to calculate the contribution.</li></ul><p><strong>Returns</strong></p><p>A StaticVector{T} representing the contribution to the <code>m</code> bin of the histogram.</p><p>This function applies the <code>γ</code> function to compute the contribution of the composition of <code>ψₘ</code> and <code>ϕ(yₖ, yₙ)</code> to the <code>m</code>-th bin of the histogram. The result is a StaticVector{T} for improved performance.</p><p>Please note that although this function offers improved performance, it cannot be used in the training process with Zygote because Zygote does not support StaticArrays.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/josemanuel22/ISL/blob/38691646a98446e64cd307add9553544e578f187/src/CustomLossFunction.jl#L93-L110">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ISL.ψₘ-Union{Tuple{T}, Tuple{T, Int64}} where T&lt;:AbstractFloat" href="#ISL.ψₘ-Union{Tuple{T}, Tuple{T, Int64}} where T&lt;:AbstractFloat"><code>ISL.ψₘ</code></a> — <span class="docstring-category">Method</span></header><section><div><p><code>ψₘ(y::T, m::Int64) where {T&lt;:AbstractFloat}</code>`</p><p>Calculate the bump function centered at <code>m</code>, implemented as a Gaussian function.</p><p><strong>Arguments</strong></p><ul><li><code>y::T</code>: The input value for which to compute the bump function.</li><li><code>m::Int64</code>: The center point around which the bump function is centered.</li></ul><p><strong>Returns</strong></p><p>A floating-point value representing the bump function&#39;s value at the input <code>y</code>.</p><p>This function calculates the bump function, which is centered at the integer value <code>m</code>. It is implemented as a Gaussian function with a standard deviation of 0.1.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/josemanuel22/ISL/blob/38691646a98446e64cd307add9553544e578f187/src/CustomLossFunction.jl#L24-L38">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ISL.ϕ-Union{Tuple{T}, Tuple{Matrix{T}, T}} where T&lt;:AbstractFloat" href="#ISL.ϕ-Union{Tuple{T}, Tuple{Matrix{T}, T}} where T&lt;:AbstractFloat"><code>ISL.ϕ</code></a> — <span class="docstring-category">Method</span></header><section><div><p>ϕ(yₖ::Matrix{T}, yₙ::T) where {T&lt;:AbstractFloat}</p><p>Calculate the sum of sigmoid functions centered at <code>yₙ</code> applied to the vector <code>yₖ</code>.</p><p><strong>Arguments</strong></p><ul><li><code>yₖ::Matrix{T}</code>: A matrix of values for which to compute the sum of sigmoid functions.</li><li><code>yₙ::T</code>: The center value around which the sigmoid functions are centered.</li></ul><p><strong>Returns</strong></p><p>A floating-point value representing the sum of sigmoid-transformed values.</p><p>This function calculates the sum of sigmoid functions, each centered at the value <code>yₙ</code>, applied element-wise to the matrix <code>yₖ</code>. The sum is computed according to the formula:</p><p><code>math ϕ(yₖ, yₙ) = ∑_{i=1}^K σ(yₖ^i, yₙ)</code>`</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/josemanuel22/ISL/blob/38691646a98446e64cd307add9553544e578f187/src/CustomLossFunction.jl#L44-L61">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="gan/">GAN »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Sunday 11 February 2024 16:50">Sunday 11 February 2024</span>. Using Julia version 1.10.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
