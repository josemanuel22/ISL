var documenterSearchIndex = {"docs":
[{"location":"gan/#GAN","page":"GAN","title":"GAN","text":"","category":"section"},{"location":"gan/","page":"GAN","title":"GAN","text":"In this repository, we have included a folder with different generative adversarial networks, GANs: vanilla GAN, WGAN, MMD-GAN.","category":"page"},{"location":"gan/","page":"GAN","title":"GAN","text":"In the first two cases, we have used the implementation from this repository, with some minor changes. In the last case, we have rewritten the original code written in Python to Julia.","category":"page"},{"location":"gan/","page":"GAN","title":"GAN","text":"The goal is to test that the AdapativeBlockLearning methods can work as regularizers for the solutions proposed by the GANs, providing a solution to the Helvetica scenario.","category":"page"},{"location":"benchmark/#Benchmark","page":"Benchmark","title":"Benchmark","text":"","category":"section"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"This module contains utility functions for conducting tests and generating graphs and statistics from the obtained results.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = ISL","category":"page"},{"location":"#ISL","page":"Home","title":"ISL","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for ISL.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [ISL]\nOrder   = [:module, :constant, :type, :function]","category":"page"},{"location":"#ISL.AutoISLParams","page":"Home","title":"ISL.AutoISLParams","text":"AutoISLParams\n\nHyperparameters for the method invariant_statistical_loss\n\n@with_kw struct AutoISLParams\n    samples::Int64 = 1000\n    epochs::Int64 = 100\n    η::Float64 = 1e-3\n    max_k::Int64 = 10\n    transform = Normal(0.0f0, 1.0f0)\nend;\n\n\n\n\n\n","category":"type"},{"location":"#ISL.HyperParamsTS","page":"Home","title":"ISL.HyperParamsTS","text":"HyperParamsTS\n\nHyperparameters for the method ts_adaptative_block_learning\n\n\n\n\n\n","category":"type"},{"location":"#ISL.ISLParams","page":"Home","title":"ISL.ISLParams","text":"ISLParams\n\nHyperparameters for the method adaptative_block_learning\n\n@with_kw struct ISLParams\n    samples::Int64 = 1000               # number of samples per histogram\n    K::Int64 = 2                        # number of simulted observations\n    epochs::Int64 = 100                 # number of epochs\n    η::Float64 = 1e-3                   # learning rate\n    transform = Normal(0.0f0, 1.0f0)    # transform to apply to the data\nend;\n\n\n\n\n\n","category":"type"},{"location":"#ISL._sigmoid-Union{Tuple{T}, Tuple{Matrix{T}, T}} where T<:AbstractFloat","page":"Home","title":"ISL._sigmoid","text":"_sigmoid(ŷ::Matrix{T}, y::T) where {T<:AbstractFloat}\n\nCalculate the sigmoid function centered at y.\n\nArguments\n\nŷ::Matrix{T}: The matrix of values to apply the sigmoid function to.\ny::T: The center value around which the sigmoid function is centered.\n\nReturns\n\nA matrix of the same size as ŷ containing the sigmoid-transformed values.\n\nThis function calculates the sigmoid function for each element in the matrix ŷ centered at the value y. It applies a fast sigmoid transformation with a scaling factor of 10.0.\n\n\n\n\n\n","category":"method"},{"location":"#ISL.auto_invariant_statistical_loss-Tuple{Any, Any, Any}","page":"Home","title":"ISL.auto_invariant_statistical_loss","text":"auto_invariant_statistical_loss(model, data, hparams)\n\nCustom loss function for the model.\n\nThis method gradually adapts K (starting from 2) up to max_k (inclusive). The value of K is chosen based on a simple two-sample test between the histogram associated with the obtained result and the uniform distribution.\n\nTo see the value of K used in the test, set the logger level to debug before executing.\n\n#Arguments\n\nmodel::Flux.Chain: is a Flux neuronal network model\ndata::Flux.DataLoader: is a loader Flux object\nhparams::AutoAdaptativeHyperParams: is a AutoAdaptativeHyperParams object\n\n\n\n\n\n","category":"method"},{"location":"#ISL.convergence_to_uniform-Union{Tuple{Vector{T}}, Tuple{T}} where T<:Int64","page":"Home","title":"ISL.convergence_to_uniform","text":"`convergence_to_uniform(aₖ)``\n\nTest the convergence of the distributino of the window of the rv's Aₖ to a uniform distribution. It is implemented using a Chi-Square test.\n\n\n\n\n\n","category":"method"},{"location":"#ISL.generate_aₖ-Union{Tuple{T}, Tuple{Matrix{T}, T}} where T<:AbstractFloat","page":"Home","title":"ISL.generate_aₖ","text":"generate_aₖ(ŷ, y)\n\nCalculate the values of the real observation y in each of the components of the approximate histogram with K bins.\n\nArguments\n\nŷ::Matrix{T}: A matrix of simulated observations (each column represents a different simulation).\ny::T: The real data for which the one-step histogram is generated.\n\nReturns\n\naₖ::Vector{Float}: A vector with the values of the real observation y in each of the components of the approximate histogram with K bins.\n\nDetails\n\nThe generate_aₖ function calculates the one-step histogram aₖ as the sum of the contribution of the observation to the subrogate histogram bins. It uses the function γ to calculate the contribution of each observation at each histogram bin. The final histogram is the sum of these contributions.\n\nThe formula for generating aₖ is as follows:\n\naₖ = _k=0^K γ(y y k) = _k=0^K _i=1^N ψₖ(y yᵢ)\n\n\n\n\n\n","category":"method"},{"location":"#ISL.get_window_of_Aₖ-Tuple{Any, Any, Any, Int64}","page":"Home","title":"ISL.get_window_of_Aₖ","text":"get_window_of_Aₖ(model, target , K, n_samples)\n\nGenerate a window of the rv's Aₖ for a given model and target function.\n\n\n\n\n\n","category":"method"},{"location":"#ISL.invariant_statistical_loss-Tuple{Any, Any, Any}","page":"Home","title":"ISL.invariant_statistical_loss","text":"invariant_statistical_loss(model, data, hparams)\n\nCustom loss function for the model. model is a Flux neuronal network model, data is a loader Flux object and hparams is a HyperParams object.\n\nArguments\n\nnn_model::Flux.Chain: is a Flux neuronal network model\ndata::Flux.DataLoader: is a loader Flux object\nhparams::HyperParams: is a HyperParams object\n\n\n\n\n\n","category":"method"},{"location":"#ISL.jensen_shannon_∇-Union{Tuple{Vector{T}}, Tuple{T}} where T<:AbstractFloat","page":"Home","title":"ISL.jensen_shannon_∇","text":"jensen_shannon_∇(aₖ)\n\nJensen shannon difference between aₖ vector and uniform distribution vector.\n\n\n\n\n\n","category":"method"},{"location":"#ISL.scalar_diff-Union{Tuple{Vector{T}}, Tuple{T}} where T<:AbstractFloat","page":"Home","title":"ISL.scalar_diff","text":"scalar_diff(q)\n\nScalar difference between the vector representing our subrogate histogram and the uniform distribution vector.\n\nloss = q-1k+1_2 = _k=0^K (qₖ - 1K+1)^2\n\n\n\n\n\n","category":"method"},{"location":"#ISL.ts_invariant_statistical_loss-NTuple{5, Any}","page":"Home","title":"ISL.ts_invariant_statistical_loss","text":"ts_invariant_statistical_loss(rec, gen, Xₜ, Xₜ₊₁, hparams)\n\nTrain a model for time series data with statistical invariance loss method.\n\n#Arguments\n\nrec: The recurrent neural network (RNN) responsible for encoding the time series data.\ngen: The generative model used for generating future time series data.\nXₜ: An array of input time series data at time t.\nXₜ₊₁: An array of target time series data at time t+1.\nhparams::NamedTuple: A structure containing hyperparameters for training. It should include the following fields:\nη::Float64: Learning rate for optimization.\nwindow_size::Int: Size of the sliding window used during training.\nK::Int: Number of samples in the generative model.\nnoise_model: Noise model used for generating random noise.\n\n#Returns\n\nlosses::Vector{Float64}: A vector containing the training loss values for each iteration.\n\n#Description This function train a model for time series data with statistical invariance loss method. It utilizes a recurrent neural network (rec) to encode the time series data at time t and a generative model (gen) to generate future time series data at time t+1. The training process involves optimizing both the rec and gen models.\n\nThe function iterates through the provided time series data (Xₜ and Xₜ₊₁) in batches, with a sliding window of size window_size.\n\n\n\n\n\n","category":"method"},{"location":"#ISL.γ-Union{Tuple{T}, Tuple{Matrix{T}, T, Int64}} where T<:AbstractFloat","page":"Home","title":"ISL.γ","text":"γ(yₖ::Matrix{T}, yₙ::T, m::Int64) where {T<:AbstractFloat}\n\nCalculate the contribution of ψₘ ∘ ϕ(yₖ, yₙ) to the m bin of the histogram as a Vector{Float}.\n\nArguments\n\nyₖ::Matrix{T}: A matrix of values for which to compute the contribution.\nyₙ::T: The center value around which the sigmoid and bump functions are centered.\nm::Int64: The bin index for which to calculate the contribution.\n\nReturns\n\nA vector of floating-point values representing the contribution to the m bin of the histogram.\n\nThis function calculates the contribution of the composition of ψₘ and ϕ(yₖ, yₙ) to the m-th bin of the histogram. The result is a vector of floating-point values.\n\nThe contribution is computed according to the formula:\n\nγ(yₖ yₙ m) = ψₘ  ϕ(yₖ yₙ)\n\n\n\n\n\n","category":"method"},{"location":"#ISL.ψₘ-Union{Tuple{T}, Tuple{T, Int64}} where T<:AbstractFloat","page":"Home","title":"ISL.ψₘ","text":"ψₘ(y::T, m::Int64) where {T<:AbstractFloat}\n\nCalculate the bump function centered at m, implemented as a Gaussian function.\n\nArguments\n\ny::T: The input value for which to compute the bump function.\nm::Int64: The center point around which the bump function is centered.\n\nReturns\n\nA floating-point value representing the bump function's value at the input y.\n\nThis function calculates the bump function, which is centered at the integer value m. It is implemented as a Gaussian function with a standard deviation of 0.1.\n\n\n\n\n\n","category":"method"},{"location":"#ISL.ϕ-Union{Tuple{T}, Tuple{Matrix{T}, T}} where T<:AbstractFloat","page":"Home","title":"ISL.ϕ","text":"ϕ(yₖ::Matrix{T}, yₙ::T) where {T<:AbstractFloat}\n\nCalculate the sum of sigmoid functions centered at yₙ applied to the vector yₖ.\n\nArguments\n\nyₖ::Matrix{T}: A matrix of values for which to compute the sum of sigmoid functions.\nyₙ::T: The center value around which the sigmoid functions are centered.\n\nReturns\n\nA floating-point value representing the sum of sigmoid-transformed values.\n\nThis function calculates the sum of sigmoid functions, each centered at the value yₙ, applied element-wise to the matrix yₖ. The sum is computed according to the formula:\n\nmath ϕ(yₖ, yₙ) = ∑_{i=1}^K σ(yₖ^i, yₙ)`\n\n\n\n\n\n","category":"method"},{"location":"example/","page":"Example","title":"Example","text":"To make simple use, once the package is installed, just run the examples in examples/ directory.","category":"page"},{"location":"example/#Learning-1-D-distributions","page":"Example","title":"Learning 1-D distributions","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"# This example is from examples/Learning1d_distribution/benchmark_unimodal.jl\n# We include the module\nusing ISL\ninclude(\"../utils.jl\")\n\n@test_experiments \"N(0,1) to N(23,1)\" begin\n    # Generator: a neural network with ELU activation\n    gen = Chain(Dense(1, 7), elu, Dense(7, 13), elu, Dense(13, 7), elu, Dense(7, 1))\n\n    # Discriminator: a neural network with ELU activation and σ activation function in the last layer\n    dscr = Chain(\n        Dense(1, 11), elu, Dense(11, 29), elu, Dense(29, 11), elu, Dense(11, 1, σ)\n    )\n\n    #Noise model\n    noise_model = Normal(0.0f0, 1.0f0)\n    n_samples = 10000   \n    # Target model composed of a mixture of models\n    target_model = Normal(4.0f0, 2.0f0)\n\n    # Parameters for automatic invariant statistical loss\n    hparams = AutoISLParams(;\n        max_k=10, samples=1000, epochs=1000, η=1e-2, transform=noise_model\n    )\n\n    # Preparing the training set and data loader\n    train_set = Float32.(rand(target_model, hparams.samples))\n    loader = Flux.DataLoader(train_set; batchsize=-1, shuffle=true, partial=false)\n\n    # Training using the automatic invariant statistical loss\n    auto_invariant_statistical_loss(gen, loader, hparams)\n\n    #We plot the results\n    plot_global(\n        x -> quantile.(-target_model, cdf(noise_model, x)),\n        noise_model,\n        target_model,\n        gen,\n        n_samples,\n        (-3:0.1:3),\n        (-2:0.1:10),\n    )\nend","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"(Image: Example Image)","category":"page"},{"location":"example/#Time-Series","page":"Example","title":"Time Series","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"@test_experiments \"testing AutoRegressive Model 1\" begin\n    # --- Model Parameters and Data Generation ---\n\n    # Define AR model parameters\n    ar_hparams = ARParams(;\n        ϕ=[0.5f0, 0.3f0, 0.2f0],  # Autoregressive coefficients\n        x₁=rand(Normal(0.0f0, 1.0f0)),  # Initial value from a Normal distribution\n        proclen=2000,  # Length of the process\n        noise=Normal(0.0f0, 0.2f0),  # Noise in the AR process\n    )\n\n    # Define the recurrent and generative models\n    recurrent_model = Chain(RNN(1 => 10, relu), RNN(10 => 10, relu))\n    generative_model = Chain(Dense(11, 16, relu), Dense(16, 1, identity))\n\n    # Generate training and testing data\n    n_series = 200  # Number of series to generate\n    loaderXtrain, loaderYtrain, loaderXtest, loaderYtest = generate_batch_train_test_data(\n        n_series, ar_hparams\n    )\n\n    # --- Training Configuration ---\n\n    # Define hyperparameters for time series prediction\n    ts_hparams = HyperParamsTS(;\n        seed=1234,\n        η=1e-3,  # Learning rate\n        epochs=n_series,\n        window_size=1000,  # Size of the window for prediction\n        K=10,  # Hyperparameter K (if it has a specific use, add a comment)\n    )\n\n    # Train model and calculate loss\n    loss = ts_invariant_statistical_loss_one_step_prediction(\n        recurrent_model, generative_model, loaderXtrain, loaderYtrain, ts_hparams\n    )\n\n    # --- Visualization ---\n\n    # Plotting the time series prediction\n    plot_univariate_ts_prediction(\n        recurrent_model,\n        generative_model,\n        collect(loaderXtrain)[2],  # Extract the first batch for plotting\n        collect(loaderXtest)[2],  # Extract the first batch for plotting\n        ts_hparams;\n        n_average=1000,  # Number of predictions to average\n    )\nend","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"(Image: Example Image)","category":"page"}]
}
